{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:16829df75dd70ef165d96337ac9a71dca06b9e8ce7765b0a9270a3a01c4df03e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest\n",
      "## A Python testing framework\n",
      "## http://pytest.org\n",
      "\n",
      "Austin Godber\n",
      "@godber\n",
      "\n",
      "DesertPy - 6/25/2014"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Testing\n",
      "\n",
      "What, Why and Where?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Does your code do what it should?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Does it continue to do what it should after you modify it?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Three months later, how do you remember what it was supposed to do anyway?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Tests can help *while* you're writing the code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Remember\n",
      "\n",
      "    Always code as if the guy who ends up\n",
      "    maintaining your code will be a violent\n",
      "    psychopath who knows where you live.\n",
      "    \n",
      "    Code for readability."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "I am sure the psychopath will appreciate a working test suite too."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Testing Types\n",
      "\n",
      "* **unit testing** - test functionality of individual procedures\n",
      "* **integration testing** - test how parts work together (interfaces)\n",
      "* **system testing** - testing the whole system at a high level (black box, functional)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Testing in Python\n",
      "\n",
      "* [unittest](https://docs.python.org/2/library/unittest.html) - assertion based\n",
      "* [doctest](https://docs.python.org/2/library/doctest.html) - example based, integrated with docstrings\n",
      "* [nose](https://nose.readthedocs.org/en/latest/) - assertion based tests with framework and plugins\n",
      "* [pytest](http://pytest.org/latest/) - framework that supports all of the above and more"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# pytest\n",
      "\n",
      "* Pytest can run **unittest**, **doctest** and **nose** style test suites\n",
      "* no-boilerplate\n",
      "* quick to get started, powerful to do more complex things\n",
      "* plugins\n",
      "* I started with **nose** and switched to **pytest**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Basic Test\n",
      "\n",
      "In its simplest form, a **pytest** test is a function with test in the name and will be found automatically if test is in in the filename (details later)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile test_my_add.py\n",
      "def my_add(a,b):\n",
      "    return a + b\n",
      "\n",
      "def test_my_add():\n",
      "    assert my_add(2,3) == 5"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting test_my_add.py\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Running the basic test\n",
      "\n",
      "Run the following command in the directory with the file: \n",
      "\n",
      "``py.test``\n",
      "\n",
      "The output would look like this:\n",
      "\n",
      "<img src=\"pytest-basic-output.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Failed Test Output\n",
      "\n",
      "<img src=\"pytest-basic-output-fail.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest Test Discovery\n",
      "\n",
      "How does `pytest` find its tests?\n",
      "\n",
      "* Collection starts from the initial command line\n",
      "* Recurse into directories, unless they match `norecursedirs`\n",
      "* `test_*.py` or `*_test.py` files, imported by their package name.\n",
      "* Classes prefixed with `Test` (without an `__init__` method)\n",
      "* Functions or methods prefixed with `test_`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Explaining the Basic Example\n",
      "\n",
      "* Basic example file was `test_my_add.py`\n",
      "* Executed by running: `py.test`\n",
      "* Had it been `my_add.py` \n",
      "* Execute by running `py.test my_add.py`."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest and Doctest\n",
      "\n",
      "* By default, `pytest` only runs doctests in `*.txt` files\n",
      "* Add more with `--doctest-glob='*.rst'`\n",
      "* Run doctests in module docstrings with: `py.test --doctest-modules`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Two tests: a doctest and assertion based unit test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile test_my_add2.py\n",
      "def my_add(a,b):\n",
      "    \"\"\" Sample doctest\n",
      "    >>> my_add(3,4)\n",
      "    7\n",
      "    \"\"\"\n",
      "    return a + b\n",
      "\n",
      "def test_my_add():\n",
      "    assert my_add(2,3) == 5"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting test_my_add2.py\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test test_my_add2.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 1 items\u001b[0m\u001b[1m\r",
        "collected 1 items \r\n",
        "\u001b[0m\r\n",
        "test_my_add2.py .\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m=========================== 1 passed in 0.01 seconds ===========================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "only one test ran!?!?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test test_my_add2.py --doctest-modules"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 1 items\u001b[0m\u001b[1m\r",
        "collecting 2 items\u001b[0m\u001b[1m\r",
        "collected 2 items \r\n",
        "\u001b[0m\r\n",
        "test_my_add2.py ..\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m=========================== 2 passed in 0.04 seconds ===========================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "much better ... two run with **--doctest-modules** argument"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest and Python unittest\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile test_my_add3.py\n",
      "import unittest\n",
      "\n",
      "def my_add(a,b):\n",
      "    return a + b\n",
      "\n",
      "class TestMyAdd(unittest.TestCase):\n",
      "    def test_add1(self):\n",
      "        assert my_add(3,4), 7"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing test_my_add3.py\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test test_my_add3.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 1 items\u001b[0m\u001b[1m\r",
        "collecting 1 items\u001b[0m\u001b[1m\r",
        "collected 1 items \r\n",
        "\u001b[0m\r\n",
        "test_my_add3.py .\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m=========================== 1 passed in 0.03 seconds ===========================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "runs as you would expect, without modification."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Reflection\n",
      "\n",
      "Just to be clear ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* use of **doctest** and **unittest** are optional"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* legacy perhaps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* tests can be implemented as shown in the base example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* mix and match even"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Split your tests out into a separate file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "\n",
      "# Into a `test/` subdirectory even"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Like so:\n",
      "\n",
      "<img src=\"py-module-test-layout.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest Assertions\n",
      "\n",
      "* Python `assert` statement\n",
      "* Expecting exceptions: `pytest.raises()`\n",
      "* Expecting failure (coming up)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# New Example\n",
      "\n",
      "The following `badstats` module will be used in upcoming examples:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/badstats/__init__.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _sum(data):\n",
      "    total = 0\n",
      "    for d in data:\n",
      "        total += d\n",
      "    return total\n",
      "\n",
      "def mean(data):\n",
      "    n = len(data)\n",
      "    return _sum(data) / n\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/tests/test_badstats_sum_a.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from badstats import _sum\n",
      "\n",
      "def test_sum_simple():\n",
      "    data = (1, 2, 3, 4)\n",
      "    assert _sum(data) == 10\n",
      "\n",
      "def test_sum_fails():\n",
      "    data = (1.2, -1.0)\n",
      "    assert _sum(data) == 0.2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test badstats/tests/test_badstats_sum_a.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 2 items\u001b[0m\u001b[1m\r",
        "collected 2 items \r\n",
        "\u001b[0m\r\n",
        "badstats/tests/test_badstats_sum_a.py .F\r\n",
        "\r\n",
        "=================================== FAILURES ===================================\r\n",
        "________________________________ test_sum_fails ________________________________\r\n",
        "\r\n",
        "\u001b[1m    def test_sum_fails():\u001b[0m\r\n",
        "\u001b[1m        data = (1.2, -1.0)\u001b[0m\r\n",
        "\u001b[1m>       assert _sum(data) == 0.2\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE       assert 0.19999999999999996 == 0.2\u001b[0m\r\n",
        "\u001b[1m\u001b[31mE        +  where 0.19999999999999996 = _sum((1.2, -1.0))\u001b[0m\r\n",
        "\r\n",
        "badstats/tests/test_badstats_sum_a.py:9: AssertionError\r\n",
        "\u001b[1m\u001b[31m====================== 1 failed, 1 passed in 0.01 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Floating Point is Hard :-/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Fix it later\n",
      "* keep the test as a reminder\n",
      "* but mark it as expected to fail with `xfail`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/tests/test_badstats_sum_b.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pytest\n",
      "from badstats import _sum\n",
      "\n",
      "def test_sum_simple():\n",
      "    data = (1, 2, 3, 4)\n",
      "    assert _sum(data) == 10\n",
      "\n",
      "@pytest.mark.xfail\n",
      "def test_sum_fails():\n",
      "    data = (1.2, -1.0)\n",
      "    assert _sum(data) == 0.2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test badstats/tests/test_badstats_sum_b.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 2 items\u001b[0m\u001b[1m\r",
        "collected 2 items \r\n",
        "\u001b[0m\r\n",
        "badstats/tests/test_badstats_sum_b.py .x\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m===================== 1 passed, 1 xfailed in 0.01 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest Xfail, Conditions and More\n",
      "\n",
      "* Modfying test execution using `pytest` module\n",
      "* Use of the decorator `@pytest.mark.xfail`.\n",
      "* Skip tests with conditionals and `@pytest.mark.skipif`\n",
      "* Custom Markers: `@pytest.mark.NAME`\n",
      "  * `@pytest.mark.webtest`\n",
      "  * `py.test -v -m webtest`\n",
      "* See [mark documentation](http://pytest.org/latest/mark.html)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "# Extended Xfail Example\n",
      "\n",
      "* Variable Use\n",
      "* Xfail conditional (and reason)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/tests/test_badstats_sum_c.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pytest\n",
      "import sys\n",
      "from badstats import _sum\n",
      "xfail = pytest.mark.xfail\n",
      "\n",
      "def test_sum_simple():\n",
      "    data = (1, 2, 3, 4)\n",
      "    assert _sum(data) == 10\n",
      "\n",
      "@xfail(sys.platform == 'linux2', reason='requires windows')\n",
      "def test_sum_fails():\n",
      "    data = (1.2, -1.0)\n",
      "    assert _sum(data) == 0.2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test badstats/tests/test_badstats_sum_c.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 2 items\u001b[0m\u001b[1m\r",
        "collected 2 items \r\n",
        "\u001b[0m\r\n",
        "badstats/tests/test_badstats_sum_c.py .x\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m===================== 1 passed, 1 xfailed in 0.01 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Parametrize\n",
      "\n",
      "When you want to test a number of inputs on the same function, use `@pytest.mark.parametrize`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/tests/test_badstats_sum_param.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pytest\n",
      "from badstats import _sum\n",
      "\n",
      "@pytest.mark.parametrize(\"input,expected\", [\n",
      "    ((1, 2, 3, 4), 10),\n",
      "    ((0, 0, 1, 5), 6),\n",
      "    pytest.mark.xfail(((1.2, -1.0), 0.2)),\n",
      "])\n",
      "def test_sum(input, expected):\n",
      "    assert _sum(input) == expected\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test badstats/tests/test_badstats_sum_param.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 3 items\u001b[0m\u001b[1m\r",
        "collected 3 items \r\n",
        "\u001b[0m\r\n",
        "badstats/tests/test_badstats_sum_param.py ..x\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m===================== 2 passed, 1 xfailed in 0.01 seconds ======================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Note there appear to have been three tests executed.\n",
      "\n",
      "You can inspect what tests are 'found' by using the `py.test --collect-only`.\n",
      "\n",
      "<img src='pytest-collect-only.png'>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Pytest Fixtures\n",
      "\n",
      "\"The purpose of test fixtures is to provide a fixed baseline upon which tests can reliably and repeatedly execute. pytest fixtures offer dramatic improvements over the classic xUnit style of setup/teardown functions.\"\n",
      "\n",
      "However, [classic xUnit style Setup/teardown functions](http://pytest.org/latest/xunit_setup.html) are still available."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load badstats/tests/test_badstats_fixture.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pytest\n",
      "import badstats\n",
      "\n",
      "@pytest.fixture\n",
      "def data():\n",
      "    return (1.0, 2.0, 3.0, 4.0)\n",
      "\n",
      "def test_sum_simple(data):\n",
      "    assert badstats._sum(data) == 10.0\n",
      "\n",
      "def test_mean_simple(data):\n",
      "    assert badstats.mean(data) == 2.5\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!py.test badstats/tests/test_badstats_fixture.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
        "platform linux2 -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2\r\n",
        "\u001b[1m\r",
        "collecting 0 items\u001b[0m\u001b[1m\r",
        "collecting 2 items\u001b[0m\u001b[1m\r",
        "collected 2 items \r\n",
        "\u001b[0m\r\n",
        "badstats/tests/test_badstats_fixture.py ..\r\n",
        "\r\n",
        "\u001b[32m\u001b[1m=========================== 2 passed in 0.01 seconds ===========================\u001b[0m\r\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Running Tests on Module Code\n",
      "\n",
      "Environment setup can differ somewhat, but if you wanted to run the code shown above you have two choices:\n",
      "\n",
      "* Install `badstats` module with `pip install -e .`\n",
      "* Run pytest in `badstats` top directory with PYTHONPATH set: `PYTHONPATH=. py.test`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Other Things\n",
      "\n",
      "* Stand Alone Self Encapsulated Tests: `py.test --genscript=runtests.py`\n",
      "* Stop output capture (see `print()` statements): `py.test -s`\n",
      "* "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Advanced Topics for Lightning Talks\n",
      "\n",
      "* Grouping Tests in Classes\n",
      "* [xUnit Style Setup/Teardown](http://pytest.org/latest/xunit_setup.html)\n",
      "* Understanding Fixture Details: (scope, finalize, parameterization)\n",
      "* Integration with other tools: **`tox`**, `setuptools`\n",
      "* Advanced Reporting\n",
      "* Handling Command Line Options\n",
      "* Plugins and configuration using `conftest.py`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Thank You\n",
      "\n",
      "Austin Godber **@godber**\n",
      "\n",
      "http://desertpy.com/pages/presentations.html"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}